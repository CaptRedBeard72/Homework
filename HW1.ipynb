{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqkZDuEmOsE4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(loss_fn, initial_params, learning_rate, num_iterations, gradient_method='explicit'):\n",
        "    \"\"\"\n",
        "    Perform gradient descent optimization.\n",
        "\n",
        "    Parameters:\n",
        "    - loss_fn: A function that computes the loss and its gradient. It should accept a parameter vector and return the loss and gradient as separate arrays.\n",
        "    - initial_params: Initial parameter vector.\n",
        "    - learning_rate: Learning rate for gradient descent.\n",
        "    - num_iterations: Number of iterations to run gradient descent.\n",
        "    - gradient_method: 'explicit' for using the explicit gradient formula, 'approximate' for gradient estimation.\n",
        "\n",
        "    Returns:\n",
        "    - params: The optimized parameter vector.\n",
        "    - losses: A list of loss values at each iteration.\n",
        "    \"\"\"\n",
        "    params = np.array(initial_params)  # Ensure params is a NumPy array\n",
        "    losses = []\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        loss, gradient = loss_fn(params)\n",
        "        params -= learning_rate * gradient  # Ensure gradient is a NumPy array\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Print progress at each iteration\n",
        "        print(f\"Iteration {iteration + 1}/{num_iterations}: Loss = {loss:.4f}, Parameters = {params}\")\n",
        "\n",
        "    return params, losses\n",
        "\n",
        "def estimate_gradient(loss_fn, params, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Estimate the gradient of a loss function using finite differences.\n",
        "\n",
        "    Parameters:\n",
        "    - loss_fn: A function that computes the loss. It should accept a parameter vector and return the loss as a scalar.\n",
        "    - params: Parameter vector at which to estimate the gradient.\n",
        "    - epsilon: Small perturbation for finite differences.\n",
        "\n",
        "    Returns:\n",
        "    - gradient: Estimated gradient vector.\n",
        "    \"\"\"\n",
        "    num_params = len(params)\n",
        "    gradient = np.zeros(num_params)\n",
        "\n",
        "    for i in range(num_params):\n",
        "        perturbed_params = params.copy()\n",
        "        perturbed_params[i] += epsilon\n",
        "        loss_plus = loss_fn(perturbed_params)\n",
        "        perturbed_params[i] -= 2 * epsilon\n",
        "        loss_minus = loss_fn(perturbed_params)\n",
        "        gradient[i] = (loss_plus - loss_minus) / (2 * epsilon)\n",
        "\n",
        "    return gradient\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Define a simple loss function and its gradient (explicitly).\n",
        "def simple_loss(params):\n",
        "    x, y = params\n",
        "    loss = (x**2 + y**2)\n",
        "    gradient = np.array([2 * x, 2 * y])\n",
        "    return loss, gradient\n",
        "\n",
        "# Initial parameter guess\n",
        "initial_params = np.array([1.0, 1.0])\n",
        "\n",
        "# Run gradient descent with explicit gradient calculation\n",
        "optimized_params_explicit, losses_explicit = gradient_descent(simple_loss, initial_params, learning_rate=0.1, num_iterations=100, gradient_method='explicit')\n",
        "\n",
        "# Run gradient descent with gradient estimation\n",
        "optimized_params_approx, losses_approx = gradient_descent(simple_loss, initial_params, learning_rate=0.1, num_iterations=100, gradient_method='approximate')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(0)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Define the linear regression model\n",
        "def linear_regression(X, theta):\n",
        "    return X.dot(theta)\n",
        "\n",
        "# Define the mean squared error loss function\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "# Stochastic Gradient Descent with random initial points\n",
        "def sgd_random_start(X, y, learning_rate=0.001, n_starts, n_iterations=1000):\n",
        "    best_theta = None\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for _ in range(n_starts):\n",
        "        # Randomly initialize the parameters\n",
        "        theta = np.random.randn(2, 1)\n",
        "\n",
        "        for iteration in range(n_iterations):\n",
        "            random_index = np.random.randint(len(X))\n",
        "            xi = X[random_index:random_index+1]\n",
        "            yi = y[random_index:random_index+1]\n",
        "            gradients = -2 * xi.T.dot(yi - linear_regression(xi, theta))\n",
        "            theta -= learning_rate * gradients\n",
        "\n",
        "        y_pred = linear_regression(X, theta)\n",
        "        current_loss = mean_squared_error(y, y_pred)\n",
        "\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            best_theta = theta\n",
        "\n",
        "    return best_theta\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_starts = 10\n",
        "n_iterations = 1000\n",
        "\n",
        "X_b = np.c_[np.ones((100, 1)), X]  # Add bias term\n",
        "best_theta = sgd_random_start(X_b, y, learning_rate, n_starts, n_iterations)\n",
        "\n",
        "print(\"Best Theta:\", best_theta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xig5l_CqsTsN",
        "outputId": "c40c7bef-7124-4a09-d1e6-e6d6a48aaf27"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Theta: [[4.22374805]\n",
            " [2.94546154]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "\n",
        "# Define your neural network model here\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(train_images, train_labels), (_, _) = mnist.load_data()\n",
        "train_images = train_images / 255.0\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=SGD(learning_rate=0.01),  # Initial learning rate\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define a custom learning rate schedule\n",
        "def cyclic_learning_rate(epoch, base_lr, max_lr, step_size):\n",
        "    cycle = 1 + epoch // (2 * step_size)\n",
        "    x = abs(epoch / step_size - 2 * cycle + 1)\n",
        "    lr = base_lr + (max_lr - base_lr) * max(0, 1 - x)\n",
        "    return lr\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "step_size = 5\n",
        "base_lr = 0.001\n",
        "max_lr = 0.01\n",
        "\n",
        "best_accuracy = 0.0\n",
        "for epoch in range(num_epochs):\n",
        "    # Update learning rate\n",
        "    lr = cyclic_learning_rate(epoch, base_lr, max_lr, step_size)\n",
        "    model.optimizer.lr.assign(lr)  # Update the learning rate in TensorFlow/Keras\n",
        "\n",
        "\n",
        "    # Training\n",
        "    history = model.fit(train_images, train_labels, epochs=1, verbose=1)\n",
        "\n",
        "    # Save model parameters when learning rate vanishes\n",
        "    if lr == 0:\n",
        "        model.save_weights(f'model_epoch_{epoch}.h5')\n",
        "\n",
        "    # Validation (you need to define the validation dataset and validation loop)\n",
        "    # Compute validation accuracy and update best_accuracy if needed\n",
        "\n",
        "    accuracy = history.history['accuracy'][0]\n",
        "    print(f'Epoch [{epoch}/{num_epochs}] LR: {lr:.5f} Loss: {history.history[\"loss\"][0]:.4f} Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Load the best model parameters observed during training\n",
        "best_model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "best_model.compile(optimizer=SGD(learning_rate=0.01),\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "best_model.load_weights('best_model.h5')\n",
        "\n",
        "# Now you can use the best_model for inference or further evaluation.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "5t1VfeMryOAY",
        "outputId": "d0b54942-d5a3-4fff-ff07-bf4bdc0b3cb1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.7255 - accuracy: 0.5523\n",
            "Epoch [0/10] LR: 0.00100 Loss: 1.7255 Accuracy: 0.55%\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.6877 - accuracy: 0.8343\n",
            "Epoch [1/10] LR: 0.00280 Loss: 0.6877 Accuracy: 0.83%\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4009 - accuracy: 0.8898\n",
            "Epoch [2/10] LR: 0.00460 Loss: 0.4009 Accuracy: 0.89%\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3182 - accuracy: 0.9097\n",
            "Epoch [3/10] LR: 0.00640 Loss: 0.3182 Accuracy: 0.91%\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2716 - accuracy: 0.9225\n",
            "Epoch [4/10] LR: 0.00820 Loss: 0.2716 Accuracy: 0.92%\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2347 - accuracy: 0.9327\n",
            "Epoch [5/10] LR: 0.01000 Loss: 0.2347 Accuracy: 0.93%\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2040 - accuracy: 0.9416\n",
            "Epoch [6/10] LR: 0.00820 Loss: 0.2040 Accuracy: 0.94%\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1836 - accuracy: 0.9473\n",
            "Epoch [7/10] LR: 0.00640 Loss: 0.1836 Accuracy: 0.95%\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1707 - accuracy: 0.9514\n",
            "Epoch [8/10] LR: 0.00460 Loss: 0.1707 Accuracy: 0.95%\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1617 - accuracy: 0.9542\n",
            "Epoch [9/10] LR: 0.00280 Loss: 0.1617 Accuracy: 0.95%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ed16ba0a7102>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m                    \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                    metrics=['accuracy'])\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Now you can use the best_model for inference or further evaluation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    565\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 567\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'best_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate some sample data\n",
        "np.random.seed(0)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.rand(100, 1)\n",
        "\n",
        "# Define the linear regression model and loss function\n",
        "def linear_regression(X, theta):\n",
        "    return X.dot(theta)\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "# Number of parameter sets and learning rate\n",
        "num_parameter_sets = 10\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1000\n",
        "\n",
        "# Initialize multiple parameter sets with different initializations\n",
        "initial_parameters = [np.random.randn(1, 1) for _ in range(num_parameter_sets)]\n",
        "converged_parameters = []\n",
        "\n",
        "# Training loop for each parameter set\n",
        "for theta_init in initial_parameters:\n",
        "    theta = theta_init.copy()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Calculate gradients\n",
        "        y_pred = linear_regression(X, theta)\n",
        "        gradient = -2 * X.T.dot(y - y_pred)\n",
        "\n",
        "        # Update parameters using gradient descent\n",
        "        theta -= learning_rate * gradient\n",
        "\n",
        "        # Check for convergence based on a threshold (e.g., small gradient norm)\n",
        "        if np.linalg.norm(gradient) < 1e-3:\n",
        "            converged_parameters.append(theta.copy())\n",
        "            break\n",
        "\n",
        "# Check if there are converged parameters before calculating the mean\n",
        "if converged_parameters:\n",
        "    # Make predictions using each set of converged parameters\n",
        "    predictions = []\n",
        "    for theta in converged_parameters:\n",
        "        y_pred = linear_regression(X, theta)\n",
        "        predictions.append(y_pred)\n",
        "\n",
        "    # Calculate final output predictions as the mean of all model predictions\n",
        "    final_predictions = np.mean(predictions, axis=0)\n",
        "\n",
        "    # Calculate the final mean squared error\n",
        "    mse = mean_squared_error(y, final_predictions)\n",
        "    print(\"Final Mean Squared Error:\", mse)\n",
        "else:\n",
        "    print(\"No converged parameters found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxKrDEfNMWEU",
        "outputId": "dc8b1828-129c-4fb4-920f-dcd36e2d5d75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No converged parameters found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MultiClassLogisticClassifier:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.classes = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Determine the unique classes in the target variable\n",
        "        self.classes = np.unique(y)\n",
        "        num_features = X.shape[1]\n",
        "        num_classes = len(self.classes)\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros((num_classes, num_features))\n",
        "        self.bias = np.zeros(num_classes)\n",
        "\n",
        "        # One-hot encode the target variable\n",
        "        y_encoded = np.zeros((len(y), num_classes))\n",
        "        for i, class_label in enumerate(self.classes):\n",
        "            y_encoded[:, i] = (y == class_label)\n",
        "\n",
        "        # Gradient Descent\n",
        "        for _ in range(self.n_iterations):\n",
        "            linear_model = np.dot(X, self.weights.T) + self.bias\n",
        "            probabilities = self.softmax(linear_model)\n",
        "\n",
        "            # Calculate the gradient of the loss\n",
        "            gradient_weights = (1 / len(X)) * np.dot(probabilities.T, X)\n",
        "            gradient_bias = (1 / len(X)) * np.sum(probabilities, axis=0)\n",
        "\n",
        "            # Update weights and bias\n",
        "            self.weights -= self.learning_rate * gradient_weights\n",
        "            self.bias -= self.learning_rate * gradient_bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights.T) + self.bias\n",
        "        probabilities = self.softmax(linear_model)\n",
        "        predicted_classes = np.argmax(probabilities, axis=1)\n",
        "\n",
        "        # Map predicted indices to class labels\n",
        "        predicted_labels = [self.classes[i] for i in predicted_classes]\n",
        "        return predicted_labels\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / exp_z.sum(axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "LITSW8gROAS-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=70)\n",
        "\n",
        "# Create and train the classifier\n",
        "classifier = MultiClassLogisticClassifier(learning_rate=0.001, n_iterations=10000)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UXbS0yZOYHR",
        "outputId": "29d6e4d3-95dc-4a0d-990a-b45562e9254b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_mse_gradient(X, y, weights, predicted_probabilities):\n",
        "    \"\"\"\n",
        "    Compute the gradient of the Mean Squared Error (MSE) loss with respect to the weights.\n",
        "\n",
        "    Parameters:\n",
        "    - X: Input features (shape: [N, K], where N is the number of samples, K is the number of features).\n",
        "    - y: True labels (one-hot encoded, shape: [N, C], where C is the number of classes).\n",
        "    - weights: Model weights (shape: [C, K]).\n",
        "    - predicted_probabilities: Predicted probabilities for each class (shape: [N, C]).\n",
        "\n",
        "    Returns:\n",
        "    - gradient: Gradient of the MSE loss with respect to the weights (shape: [C, K]).\n",
        "    \"\"\"\n",
        "    N = X.shape[0]\n",
        "    C = weights.shape[0]\n",
        "    K = weights.shape[1]\n",
        "\n",
        "    gradient = np.zeros((C, K))\n",
        "\n",
        "    for i in range(N):\n",
        "        for j in range(C):\n",
        "            error = predicted_probabilities[i, j] - y[i, j]\n",
        "            gradient[j, :] += 2 * error * predicted_probabilities[i, j] * (1 - predicted_probabilities[i, j]) * X[i, :]\n",
        "\n",
        "    gradient /= N\n",
        "\n",
        "    return gradient\n"
      ],
      "metadata": {
        "id": "O7pnfomATTsA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MultiClassLogisticClassifier:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, batch_size=None, optimizer=\"batch\"):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.classes = None\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Determine the unique classes in the target variable\n",
        "        self.classes = np.unique(y)\n",
        "        num_features = X.shape[1]\n",
        "        num_classes = len(self.classes)\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.weights = np.zeros((num_classes, num_features))\n",
        "        self.bias = np.zeros(num_classes)\n",
        "\n",
        "        if self.optimizer == \"batch\":\n",
        "            for _ in range(self.n_iterations):\n",
        "                gradient_weights, gradient_bias = self.compute_gradients(X, y)\n",
        "                self.weights -= self.learning_rate * gradient_weights\n",
        "                self.bias -= self.learning_rate * gradient_bias\n",
        "\n",
        "        elif self.optimizer == \"sgd\":\n",
        "            for _ in range(self.n_iterations):\n",
        "                for i in range(len(X)):\n",
        "                    xi, yi = X[i:i+1], y[i:i+1]\n",
        "                    gradient_weights, gradient_bias = self.compute_gradients(xi, yi)\n",
        "                    self.weights -= self.learning_rate * gradient_weights\n",
        "                    self.bias -= self.learning_rate * gradient_bias\n",
        "\n",
        "        elif self.optimizer == \"mini-batch\":\n",
        "            for _ in range(self.n_iterations):\n",
        "                for i in range(0, len(X), self.batch_size):\n",
        "                    xi, yi = X[i:i+self.batch_size], y[i:i+self.batch_size]\n",
        "                    gradient_weights, gradient_bias = self.compute_gradients(xi, yi)\n",
        "                    self.weights -= self.learning_rate * gradient_weights\n",
        "                    self.bias -= self.learning_rate * gradient_bias\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights.T) + self.bias\n",
        "        probabilities = self.softmax(linear_model)\n",
        "        predicted_classes = np.argmax(probabilities, axis=1)\n",
        "\n",
        "        # Map predicted indices to class labels\n",
        "        predicted_labels = [self.classes[i] for i in predicted_classes]\n",
        "        return predicted_labels\n",
        "\n",
        "    def softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
        "\n",
        "    def compute_gradients(self, X, y):\n",
        "        linear_model = np.dot(X, self.weights.T) + self.bias\n",
        "        probabilities = self.softmax(linear_model)\n",
        "\n",
        "        # Calculate the gradient of the loss\n",
        "        gradient_weights = (1 / len(X)) * np.dot(probabilities.T, X) - np.dot(y.T, X)\n",
        "        gradient_bias = (1 / len(X)) * np.sum(probabilities - y, axis=0)\n",
        "\n",
        "        return gradient_weights, gradient_bias\n",
        "\n",
        "classifier_batch = MultiClassLogisticClassifier(learning_rate=0.1, n_iterations=1000, optimizer=\"batch\")\n",
        "classifier_sgd = MultiClassLogisticClassifier(learning_rate=0.1, n_iterations=1000, optimizer=\"sgd\")\n",
        "classifier_mini_batch = MultiClassLogisticClassifier(learning_rate=0.1, n_iterations=1000, batch_size=32, optimizer=\"mini-batch\")\n"
      ],
      "metadata": {
        "id": "iiGVLYvUUkX6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZveKW0iUqsz"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}
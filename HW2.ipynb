{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This code demonstrates how to calculate the activated output of a neuron with a PReLU activation function and determine the gradients with respect to the weights and the learnable PReLU parameter. It's a simple example to illustrate the concept of PReLU and gradient computation in a neural network context."
      ],
      "metadata": {
        "id": "gHvqHmKYk2OP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the inputs, weights, and PReLU parameter\n",
        "x1, x2, x3 = 1.0, 2.0, -1.0\n",
        "w1, w2, w3 = 0.5, -1.0, 2.0\n",
        "alpha = 0.2\n",
        "\n",
        "# Calculate the weighted sum\n",
        "z = w1 * x1 + w2 * x2 + w3 * x3\n",
        "\n",
        "# Activation function with PReLU\n",
        "def prelu(z, alpha):\n",
        "    if z > 0:\n",
        "        return z\n",
        "    else:\n",
        "        return alpha * z\n",
        "\n",
        "# Calculate the activated output\n",
        "a = prelu(z, alpha)\n",
        "\n",
        "# Calculate the gradient with respect to weights\n",
        "grad_w1 = 1 if z > 0 else alpha\n",
        "grad_w2 = 1 if z > 0 else alpha\n",
        "grad_w3 = 1 if z > 0 else alpha\n",
        "\n",
        "# Calculate the gradient with respect to the PReLU parameter Î±\n",
        "grad_alpha = min(0, z)\n",
        "\n",
        "# Print the results\n",
        "print(\"Activated Output (a):\", a)\n",
        "print(\"Gradient with respect to w1:\", grad_w1)\n",
        "print(\"Gradient with respect to w2:\", grad_w2)\n",
        "print(\"Gradient with respect to w3:\", grad_w3)\n",
        "print(\"Gradient with respect to alpha:\", grad_alpha)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X6-ap7R6DV5",
        "outputId": "17ab6ec5-27c0-4aac-d82c-ca92973fb29b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activated Output (a): -0.7000000000000001\n",
            "Gradient with respect to w1: 0.2\n",
            "Gradient with respect to w2: 0.2\n",
            "Gradient with respect to w3: 0.2\n",
            "Gradient with respect to alpha: -3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code used for a Dense Neural Network utilized to classify the CIFAR-10 dataset.\n",
        "\n",
        "This is a feedforward NN using SGD optimizer, MinMaxScaler for normalization, ReLU and Softmax for activation, Dropout for regularization and One-hot to encode the labels.\n",
        "This code also employs an EarlyStopping callback when the code has determined it has found the best accuracy after a set number of Epochs.\n",
        "\n",
        "I ran this code several times while adjusting the hyperperameters and have come to the conclustion that the below code has determined the best accuracy I could find within the alloted time period we were given.\n",
        "\n",
        "There are most likely more adjustments I could make based on the information we have learned in the lectures and appylying the new techniques down the line."
      ],
      "metadata": {
        "id": "kGeTcwplk1Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import HeNormal, GlorotNormal\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=1)\n",
        "x_test, x_val, y_test, y_val = train_test_split(x_val, y_val, test_size=0.2, random_state=1)\n",
        "\n",
        "# Data normalization\n",
        "# scaler = StandardScaler()\n",
        "scaler = MinMaxScaler()\n",
        "x_train = scaler.fit_transform(x_train.reshape(-1, 32*32*3))\n",
        "x_val = scaler.transform(x_val.reshape(-1, 32*32*3))\n",
        "x_test = scaler.transform(x_test.reshape(-1, 32*32*3))\n",
        "\n",
        "# There are 10 classes in CIFAR-10\n",
        "num_classes = 10\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define a function to create a neural network model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, activation='relu', input_shape=(32*32*3,)))\n",
        "    model.add(Dropout(0.2))  # Dropout for regularization\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))  # Dropout for regularization\n",
        "    model.add(Dense(10, activation='softmax'))  # Output layer for 10 classes\n",
        "    return model\n",
        "\n",
        "# Initialize SGD optimizer with a learning rate\n",
        "sgd = SGD(learning_rate=0.1)\n",
        "\n",
        "# Build the model\n",
        "model = create_model()\n",
        "\n",
        "# Modify the output layer\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Output layer for multi-class classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=sgd, loss=CategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# print a summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "maxEpoch = 100\n",
        "H = model.fit(x_train, y_train, epochs=maxEpoch, batch_size=32, validation_data=(x_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "# Assuming you have one-hot encoded labels\n",
        "# Convert predictedY and testY to integer labels\n",
        "# Print the Training Set Accuracy\n",
        "predictedY = np.argmax(model.predict(x_train), axis=1)\n",
        "trainY = np.argmax(y_train, axis=1)\n",
        "\n",
        "# Calculate classification report\n",
        "print(\"Training set accuracy\")\n",
        "print(classification_report(trainY, predictedY))\n",
        "\n",
        "# Print the Test Set Accuracy\n",
        "predictedY = np.argmax(model.predict(x_test), axis=1)\n",
        "testY = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Calculate classification report\n",
        "print(\"Test set accuracy\")\n",
        "print(classification_report(testY, predictedY))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpBpYJbfgTbu",
        "outputId": "6b5db42f-39d8-4c90-d6f9-b599817d4e5e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_16 (Dense)            (None, 128)               393344    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                110       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 402360 (1.53 MB)\n",
            "Trainable params: 402360 (1.53 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.1596 - accuracy: 0.1758 - val_loss: 2.0914 - val_accuracy: 0.2065\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 2.0359 - accuracy: 0.2243 - val_loss: 2.0129 - val_accuracy: 0.2285\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.9694 - accuracy: 0.2551 - val_loss: 1.9464 - val_accuracy: 0.2545\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.9229 - accuracy: 0.2715 - val_loss: 1.8754 - val_accuracy: 0.2825\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 9s 8ms/step - loss: 1.8932 - accuracy: 0.2827 - val_loss: 1.8698 - val_accuracy: 0.3010\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8582 - accuracy: 0.2966 - val_loss: 1.8023 - val_accuracy: 0.3295\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.8422 - accuracy: 0.3082 - val_loss: 1.8033 - val_accuracy: 0.3460\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.8184 - accuracy: 0.3197 - val_loss: 1.7674 - val_accuracy: 0.3490\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.8010 - accuracy: 0.3261 - val_loss: 1.7646 - val_accuracy: 0.3435\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7875 - accuracy: 0.3327 - val_loss: 1.7316 - val_accuracy: 0.3630\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.7742 - accuracy: 0.3407 - val_loss: 1.7298 - val_accuracy: 0.3695\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7597 - accuracy: 0.3441 - val_loss: 1.7335 - val_accuracy: 0.3595\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.7510 - accuracy: 0.3483 - val_loss: 1.8261 - val_accuracy: 0.3465\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7438 - accuracy: 0.3496 - val_loss: 1.7337 - val_accuracy: 0.3710\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.7337 - accuracy: 0.3553 - val_loss: 1.7255 - val_accuracy: 0.3835\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7279 - accuracy: 0.3632 - val_loss: 1.6877 - val_accuracy: 0.3855\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 1.7229 - accuracy: 0.3626 - val_loss: 1.7100 - val_accuracy: 0.3770\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7185 - accuracy: 0.3633 - val_loss: 1.7057 - val_accuracy: 0.3735\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.7079 - accuracy: 0.3666 - val_loss: 1.6591 - val_accuracy: 0.3950\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.7062 - accuracy: 0.3726 - val_loss: 1.7173 - val_accuracy: 0.3770\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 7s 5ms/step - loss: 1.6940 - accuracy: 0.3748 - val_loss: 1.7222 - val_accuracy: 0.3735\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6884 - accuracy: 0.3804 - val_loss: 1.6668 - val_accuracy: 0.3860\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6816 - accuracy: 0.3812 - val_loss: 1.6840 - val_accuracy: 0.3965\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6811 - accuracy: 0.3858 - val_loss: 1.6547 - val_accuracy: 0.4015\n",
            "Epoch 25/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6774 - accuracy: 0.3859 - val_loss: 1.7071 - val_accuracy: 0.3890\n",
            "Epoch 26/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6631 - accuracy: 0.3923 - val_loss: 1.6292 - val_accuracy: 0.4265\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6638 - accuracy: 0.3921 - val_loss: 1.6531 - val_accuracy: 0.4060\n",
            "Epoch 28/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6571 - accuracy: 0.3936 - val_loss: 1.6203 - val_accuracy: 0.4090\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6496 - accuracy: 0.3980 - val_loss: 1.6635 - val_accuracy: 0.3980\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6499 - accuracy: 0.3995 - val_loss: 1.6420 - val_accuracy: 0.4155\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6441 - accuracy: 0.4036 - val_loss: 1.6220 - val_accuracy: 0.4155\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6366 - accuracy: 0.4051 - val_loss: 1.6177 - val_accuracy: 0.4270\n",
            "Epoch 33/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6397 - accuracy: 0.4053 - val_loss: 1.6231 - val_accuracy: 0.4090\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6305 - accuracy: 0.4072 - val_loss: 1.6585 - val_accuracy: 0.3965\n",
            "Epoch 35/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6283 - accuracy: 0.4067 - val_loss: 1.6060 - val_accuracy: 0.4150\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6237 - accuracy: 0.4139 - val_loss: 1.6614 - val_accuracy: 0.4005\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6220 - accuracy: 0.4139 - val_loss: 1.6491 - val_accuracy: 0.4125\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6197 - accuracy: 0.4173 - val_loss: 1.6297 - val_accuracy: 0.4250\n",
            "Epoch 39/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6141 - accuracy: 0.4183 - val_loss: 1.6182 - val_accuracy: 0.4050\n",
            "Epoch 40/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.6140 - accuracy: 0.4168 - val_loss: 1.6245 - val_accuracy: 0.4325\n",
            "Epoch 41/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.6099 - accuracy: 0.4189 - val_loss: 1.6403 - val_accuracy: 0.4170\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 1.6089 - accuracy: 0.4185 - val_loss: 1.5797 - val_accuracy: 0.4410\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5996 - accuracy: 0.4216 - val_loss: 1.5728 - val_accuracy: 0.4435\n",
            "Epoch 44/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5962 - accuracy: 0.4250 - val_loss: 1.6083 - val_accuracy: 0.4205\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5918 - accuracy: 0.4279 - val_loss: 1.6334 - val_accuracy: 0.4030\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5971 - accuracy: 0.4216 - val_loss: 1.6747 - val_accuracy: 0.4015\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5936 - accuracy: 0.4243 - val_loss: 1.5986 - val_accuracy: 0.4290\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5891 - accuracy: 0.4285 - val_loss: 1.6110 - val_accuracy: 0.4350\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5772 - accuracy: 0.4317 - val_loss: 1.5961 - val_accuracy: 0.4400\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5746 - accuracy: 0.4327 - val_loss: 1.6024 - val_accuracy: 0.4180\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5783 - accuracy: 0.4330 - val_loss: 1.5838 - val_accuracy: 0.4410\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 1.5768 - accuracy: 0.4317 - val_loss: 1.6319 - val_accuracy: 0.4155\n",
            "Epoch 53/100\n",
            "1247/1250 [============================>.] - ETA: 0s - loss: 1.5758 - accuracy: 0.4350Restoring model weights from the end of the best epoch: 43.\n",
            "1250/1250 [==============================] - 6s 5ms/step - loss: 1.5756 - accuracy: 0.4348 - val_loss: 1.5745 - val_accuracy: 0.4565\n",
            "Epoch 53: early stopping\n",
            "1250/1250 [==============================] - 3s 3ms/step\n",
            "Training set accuracy\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.44      0.48      4014\n",
            "           1       0.53      0.50      0.51      4014\n",
            "           2       0.30      0.21      0.25      3962\n",
            "           3       0.30      0.46      0.36      4011\n",
            "           4       0.41      0.41      0.41      3993\n",
            "           5       0.46      0.33      0.39      4005\n",
            "           6       0.60      0.47      0.52      4013\n",
            "           7       0.62      0.58      0.60      3988\n",
            "           8       0.59      0.59      0.59      4022\n",
            "           9       0.44      0.67      0.54      3978\n",
            "\n",
            "    accuracy                           0.47     40000\n",
            "   macro avg       0.48      0.47      0.47     40000\n",
            "weighted avg       0.48      0.47      0.47     40000\n",
            "\n",
            "250/250 [==============================] - 1s 2ms/step\n",
            "Test set accuracy\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.44      0.47       793\n",
            "           1       0.48      0.46      0.47       768\n",
            "           2       0.28      0.19      0.22       842\n",
            "           3       0.28      0.46      0.35       788\n",
            "           4       0.37      0.40      0.38       810\n",
            "           5       0.45      0.32      0.37       790\n",
            "           6       0.52      0.40      0.45       777\n",
            "           7       0.57      0.52      0.55       809\n",
            "           8       0.61      0.57      0.59       794\n",
            "           9       0.42      0.63      0.51       829\n",
            "\n",
            "    accuracy                           0.44      8000\n",
            "   macro avg       0.45      0.44      0.44      8000\n",
            "weighted avg       0.45      0.44      0.43      8000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}